{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from lightrag.utils import setup_logger\n",
    "from lightrag import LightRAG, QueryParam\n",
    "\n",
    "# Setup log handler for LightRAG\n",
    "setup_logger(\"lightrag\", level=\"INFO\")\n",
    "\n",
    "async def initialize_rag():\n",
    "    # Initialize Groq LLM\n",
    "    llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "\n",
    "    # Initialize HuggingFace embedding model\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=\"your/path\",\n",
    "        llm_model_func=lambda query: llm.complete(query),  # Groq-compatible completion function\n",
    "        embedding_func=EmbeddingFunc(    # HuggingFace-compatible embedding function\n",
    "            embedding_dim=384,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 49\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     45\u001b[0m             rag\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, param\u001b[38;5;241m=\u001b[39mQueryParam(mode\u001b[38;5;241m=\u001b[39mmode))\n\u001b[0;32m     46\u001b[0m         )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Initialize RAG instance\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     rag \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialize_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacts.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     40\u001b[0m         rag\u001b[38;5;241m.\u001b[39minsert(f\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\asyncio\\runners.py:191\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.llama_index_impl import llama_index_complete, llama_index_embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.utils import setup_logger, EmbeddingFunc\n",
    "from lightrag import LightRAG, QueryParam\n",
    "\n",
    "# Setup log handler for LightRAG\n",
    "setup_logger(\"lightrag\", level=\"INFO\")\n",
    "\n",
    "async def initialize_rag():\n",
    "    # Initialize the Groq LLM with your specific model and API key\n",
    "    llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "\n",
    "    # Initialize the HuggingFace embedding model with your chosen model\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=\"R:/lightrag_implemented_chatbot\",\n",
    "        llm_model_func=lambda query: llama_index_complete(query, llm=llm),  # Use Groq for completion\n",
    "        embedding_func=EmbeddingFunc(  # Use HuggingFace for embeddings\n",
    "            embedding_dim=384,  # Adjust based on your embedding model\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag\n",
    "\n",
    "def main():\n",
    "    # Initialize RAG instance\n",
    "    rag = asyncio.run(initialize_rag())\n",
    "\n",
    "    with open(\"facts.txt\", \"r\") as f:\n",
    "        rag.insert(f.read())\n",
    "\n",
    "    # Example queries with different search modes\n",
    "    for mode in [\"naive\", \"local\", \"global\", \"hybrid\"]:\n",
    "        print(\n",
    "            rag.query(\"summary\", param=QueryParam(mode=mode))\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "c:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rahul\\AppData\\Local\\llama_index\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "INFO: Process 11020 Shared-Data already initialized (multiprocess=False)\n",
      "INFO: Created new empty graph\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_chunks.json'} 0 data\n",
      "INFO: Process 11020 storage namespace already initialized: [full_docs]\n",
      "INFO: Process 11020 storage namespace already initialized: [text_chunks]\n",
      "INFO: Process 11020 storage namespace already initialized: [llm_response_cache]\n",
      "INFO: Process 11020 storage namespace already initialized: [doc_status]\n",
      "INFO: Process 11020 storage namespace already initialized: [full_docs]\n",
      "INFO: Process 11020 storage namespace already initialized: [text_chunks]\n",
      "INFO: Process 11020 storage namespace already initialized: [llm_response_cache]\n",
      "INFO: Process 11020 storage namespace already initialized: [doc_status]\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 1 records to doc_status\n",
      "INFO: Stored 1 new unique documents\n",
      "INFO: Storage Initialization completed!\n",
      "INFO: Number of batches to process: 1.\n",
      "INFO: Start processing batch 1 of 1.\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 1 records to doc_status\n",
      "INFO: Inserting 1 to chunks\n",
      "INFO: Inserting 1 records to full_docs\n",
      "INFO: Inserting 1 records to text_chunks\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "INFO: Process 11020 reloading chunks due to update by another process\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_chunks.json'} 0 data\n",
      "ERROR: Failed to extract entities and relationships\n",
      "ERROR: Failed to process document doc-ca25e1b95daef467c64a9046e8200eec: initialize_rag.<locals>.<lambda>() got an unexpected keyword argument 'hashing_kv'\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 1 records to doc_status\n",
      "INFO: Completed batch 1 of 1.\n",
      "INFO: Process 11020 KV writting 1 records to full_docs\n",
      "INFO: Process 11020 KV writting 1 records to text_chunks\n",
      "INFO: Writing graph with 0 nodes, 0 edges\n",
      "INFO: All Insert done\n",
      "INFO: Document processing pipeline completed\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I'm not able to provide an answer to that question.[no-context]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\ast.py:264: RuntimeWarning: coroutine 'LightRAG.ainsert' was never awaited\n",
      "  yield field, getattr(node, field)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "initialize_rag.<locals>.<lambda>() got an unexpected keyword argument 'hashing_kv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 53\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     49\u001b[0m             rag\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, param\u001b[38;5;241m=\u001b[39mQueryParam(mode\u001b[38;5;241m=\u001b[39mmode))\n\u001b[0;32m     50\u001b[0m         )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Run the async main function\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\asyncio\\futures.py:202\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Example queries with different search modes\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m---> 49\u001b[0m         \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQueryParam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     )\n",
      "File \u001b[1;32mr:\\lightrag_implemented_chatbot\\lightrag\\lightrag\\lightrag.py:1238\u001b[0m, in \u001b[0;36mLightRAG.query\u001b[1;34m(self, query, param, system_prompt)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03mPerform a sync query.\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m    str: The result of the query execution.\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1236\u001b[0m loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[1;32m-> 1238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\asyncio\\futures.py:202\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\envs\\chatbot\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mr:\\lightrag_implemented_chatbot\\lightrag\\lightrag\\lightrag.py:1258\u001b[0m, in \u001b[0;36mLightRAG.aquery\u001b[1;34m(self, query, param, system_prompt)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;124;03mPerform a async query.\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m    str: The result of the query execution.\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 1258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kg_query(\n\u001b[0;32m   1259\u001b[0m         query\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_entity_relation_graph,\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentities_vdb,\n\u001b[0;32m   1262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelationships_vdb,\n\u001b[0;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_chunks,\n\u001b[0;32m   1264\u001b[0m         param,\n\u001b[0;32m   1265\u001b[0m         asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   1266\u001b[0m         hashing_kv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_response_cache,  \u001b[38;5;66;03m# Directly use llm_response_cache\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m   1268\u001b[0m     )\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1270\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m naive_query(\n\u001b[0;32m   1271\u001b[0m         query\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks_vdb,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1277\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m   1278\u001b[0m     )\n",
      "File \u001b[1;32mr:\\lightrag_implemented_chatbot\\lightrag\\lightrag\\operate.py:686\u001b[0m, in \u001b[0;36mkg_query\u001b[1;34m(query, knowledge_graph_inst, entities_vdb, relationships_vdb, text_chunks_db, query_param, global_config, hashing_kv, system_prompt)\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_response\n\u001b[0;32m    685\u001b[0m \u001b[38;5;66;03m# Extract keywords using extract_keywords_only function which already supports conversation history\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m hl_keywords, ll_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_keywords_only(\n\u001b[0;32m    687\u001b[0m     query, query_param, global_config, hashing_kv\n\u001b[0;32m    688\u001b[0m )\n\u001b[0;32m    690\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh-level keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhl_keywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    691\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow-level  keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mll_keywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mr:\\lightrag_implemented_chatbot\\lightrag\\lightrag\\operate.py:839\u001b[0m, in \u001b[0;36mextract_keywords_only\u001b[1;34m(text, param, global_config, hashing_kv)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# 5. Call the LLM for keyword extraction\u001b[39;00m\n\u001b[0;32m    838\u001b[0m use_model_func \u001b[38;5;241m=\u001b[39m global_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_model_func\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 839\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m use_model_func(kw_prompt, keyword_extraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# 6. Parse out JSON from the LLM response\u001b[39;00m\n\u001b[0;32m    842\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, result, re\u001b[38;5;241m.\u001b[39mDOTALL)\n",
      "File \u001b[1;32mr:\\lightrag_implemented_chatbot\\lightrag\\lightrag\\utils.py:266\u001b[0m, in \u001b[0;36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[1;32m--> 266\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: initialize_rag.<locals>.<lambda>() got an unexpected keyword argument 'hashing_kv'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Storage Finalization completed!\n",
      "INFO: Storage Finalization completed!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio  # Add this import\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.llama_index_impl import llama_index_complete, llama_index_embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.utils import setup_logger, EmbeddingFunc\n",
    "from lightrag import LightRAG, QueryParam\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup log handler for LightRAG\n",
    "setup_logger(\"lightrag\", level=\"INFO\")\n",
    "\n",
    "async def initialize_rag():\n",
    "    # Initialize the Groq LLM with your specific model and API key\n",
    "    llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "\n",
    "    # Initialize the HuggingFace embedding model with your chosen model\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=\"R:/lightrag_implemented_chatbot\",\n",
    "        llm_model_func=lambda query: llama_index_complete(query, llm=llm),\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=384,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag\n",
    "\n",
    "async def main():  # Make main async\n",
    "    # Initialize RAG instance\n",
    "    rag = await initialize_rag()  # Use await instead of asyncio.run\n",
    "\n",
    "    with open(\"facts.txt\", \"r\") as f:\n",
    "        rag.insert(f.read())\n",
    "\n",
    "    # Example queries with different search modes\n",
    "    for mode in [\"naive\", \"local\", \"global\", \"hybrid\"]:\n",
    "        print(\n",
    "            rag.query(\"summary\", param=QueryParam(mode=mode))\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())  # Run the async main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "INFO: Process 11020 Shared-Data already initialized (multiprocess=False)\n",
      "DEBUG: LightRAG init with param:\n",
      "  working_dir = R:/lightrag_implemented_chatbot,\n",
      "  kv_storage = JsonKVStorage,\n",
      "  vector_storage = NanoVectorDBStorage,\n",
      "  graph_storage = NetworkXStorage,\n",
      "  doc_status_storage = JsonDocStatusStorage,\n",
      "  log_level = None,\n",
      "  log_file_path = None,\n",
      "  entity_extract_max_gleaning = 1,\n",
      "  entity_summary_to_max_tokens = 500,\n",
      "  chunk_token_size = 1200,\n",
      "  chunk_overlap_token_size = 100,\n",
      "  tiktoken_model_name = gpt-4o-mini,\n",
      "  chunking_func = <function chunking_by_token_size at 0x0000016AB10CA8E0>,\n",
      "  node_embedding_algorithm = node2vec,\n",
      "  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n",
      "  embedding_func = {'embedding_dim': 384, 'max_token_size': 8192, 'func': <function initialize_rag.<locals>.<lambda> at 0x0000016A8128A840>},\n",
      "  embedding_batch_num = 32,\n",
      "  embedding_func_max_async = 16,\n",
      "  embedding_cache_config = {'enabled': False, 'similarity_threshold': 0.95, 'use_llm_check': False},\n",
      "  llm_model_func = <function initialize_rag.<locals>.llm_wrapper at 0x0000016A8128BF60>,\n",
      "  llm_model_name = gpt-4o-mini,\n",
      "  llm_model_max_token_size = 32768,\n",
      "  llm_model_max_async = 16,\n",
      "  llm_model_kwargs = {},\n",
      "  vector_db_storage_cls_kwargs = {'cosine_better_than_threshold': 0.2},\n",
      "  namespace_prefix = ,\n",
      "  enable_llm_cache = True,\n",
      "  enable_llm_cache_for_entity_extract = True,\n",
      "  max_parallel_insert = 20,\n",
      "  addon_params = {'language': 'English'},\n",
      "  auto_manage_storages_states = True,\n",
      "  convert_response_to_json_func = <function convert_response_to_json at 0x0000016AB0E7D940>,\n",
      "  cosine_better_than_threshold = 0.2,\n",
      "  _storages_status = StoragesStatus.NOT_CREATED\n",
      "\n",
      "INFO: Loaded graph from R:/lightrag_implemented_chatbot\\graph_chunk_entity_relation.graphml with 61 nodes, 11 edges\n",
      "INFO:nano-vectordb:Load (53, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_entities.json'} 53 data\n",
      "INFO:nano-vectordb:Load (11, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_relationships.json'} 11 data\n",
      "INFO:nano-vectordb:Load (1, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_chunks.json'} 1 data\n",
      "INFO: Process 11020 storage namespace already initialized: [full_docs]\n",
      "INFO: Process 11020 storage namespace already initialized: [text_chunks]\n",
      "INFO: Process 11020 storage namespace already initialized: [llm_response_cache]\n",
      "INFO: Process 11020 storage namespace already initialized: [doc_status]\n",
      "INFO: Process 11020 storage namespace already initialized: [full_docs]\n",
      "INFO: Process 11020 storage namespace already initialized: [text_chunks]\n",
      "INFO: Process 11020 storage namespace already initialized: [llm_response_cache]\n",
      "INFO: Process 11020 storage namespace already initialized: [doc_status]\n",
      "DEBUG: Initialized Storages\n",
      "DEBUG: Initialized Storages\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 2 records to doc_status\n",
      "INFO: Stored 1 new unique documents\n",
      "INFO: Storage Initialization completed!\n",
      "INFO: Number of batches to process: 1.\n",
      "INFO: Start processing batch 1 of 1.\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 2 records to doc_status\n",
      "INFO: Inserting 1 to chunks\n",
      "INFO: Inserting 1 records to full_docs\n",
      "INFO: Inserting 1 records to text_chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading content (456 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "DEBUG: Non-embedding cached missed(mode:default type:extract)\n",
      "INFO: Process 11020 reloading chunks due to update by another process\n",
      "INFO:nano-vectordb:Load (1, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_chunks.json'} 1 data\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:default type:extract)\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO:   Chunk 1/1: extracted 14 entities and 15 relationships (deduplicated)\n",
      "INFO: Process 11020 reloading graph chunk_entity_relation due to update by another process\n",
      "INFO: Extracted 14 entities and 15 relationships (deduplicated)\n",
      "DEBUG: New entities:[{'entity_id': 'Financial Planning', ...\n",
      "DEBUG: New relationships:[{'src_id': 'Budgeting', 'tgt_id...\n",
      "INFO: Inserting 14 to entities\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 11.72it/s]\n",
      "INFO: Process 11020 reloading entities due to update by another process\n",
      "INFO:nano-vectordb:Load (53, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_entities.json'} 53 data\n",
      "INFO: Inserting 15 to relationships\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n",
      "INFO: Process 11020 reloading relationships due to update by another process\n",
      "INFO:nano-vectordb:Load (11, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'R:/lightrag_implemented_chatbot\\\\vdb_relationships.json'} 11 data\n",
      "INFO: Inserting 1 records to doc_status\n",
      "INFO: Process 11020 doc status writting 2 records to doc_status\n",
      "INFO: Completed batch 1 of 1.\n",
      "INFO: Process 11020 KV writting 2 records to full_docs\n",
      "INFO: Process 11020 KV writting 2 records to text_chunks\n",
      "INFO: Process 11020 KV writting 7 records to llm_response_cache\n",
      "INFO: Writing graph with 75 nodes, 26 edges\n",
      "INFO: All Insert done\n",
      "INFO: Document processing pipeline completed\n",
      "DEBUG: Non-embedding cached missed(mode:naive type:query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content inserted successfully\n",
      "\n",
      "Query: What is the recommended savings rate?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.23it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: [naive_query]Prompt Tokens: 332\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 8 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:local type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:local type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode naive: **Savings Rate Recommendation**\n",
      "\n",
      "According to the provided financial planning overview, it is recommended to set aside **20% of your monthly salary** for savings. This is an essential step in managing personal finances effectively and achieving long-term financial goals.\n",
      "\n",
      "It's worth noting that this savings rate is not only for short-term goals but also for building an emergency fund that covers at least **6 months of expenses**. This fund will provide a financial safety net in case of unexpected events or financial downturns.\n",
      "\n",
      "By following this savings rate and building an emergency fund, you'll be well on your way to achieving financial stability and security.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Savings strategy', 'Financial planning']\n",
      "DEBUG: Low-level  keywords: ['Emergency fund', 'Retirement savings', 'Interest rates', 'Budgeting']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query nodes: Emergency fund, Retirement savings, Interest rates, Budgeting, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.46it/s]\n",
      "DEBUG: Truncate relations from 15 to 15 (max tokens:4000)\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 9 to 9 (max tokens:4000)\n",
      "INFO: Local query uses 9 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 1310\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 15.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 10 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:global type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:global type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode local: **Savings Rate Recommendation**\n",
      "\n",
      "According to the provided Financial Planning Overview, it is recommended to set aside **20% of monthly salary for savings**. This is an essential step in managing personal finances effectively and achieving long-term financial goals.\n",
      "\n",
      "**Context and Importance of Savings**\n",
      "\n",
      "Savings refer to the amount of money set aside for future use, such as emergency funds or long-term investments. Having a sufficient savings rate helps individuals build an emergency fund, which is a reserve of money set aside to cover unexpected expenses or financial emergencies. This fund provides a safety net for unexpected expenses, ensuring financial stability and security.\n",
      "\n",
      "**Financial Planning and Budgeting**\n",
      "\n",
      "The recommended savings rate is part of a comprehensive financial planning process, which involves budgeting, tracking expenses, and allocating resources towards necessary expenses, savings, and investments. By following this approach, individuals can effectively manage their personal finances, achieve financial goals, and maintain a balanced financial situation.\n",
      "\n",
      "If you have any further questions or would like to know more about financial planning and budgeting, feel free to ask!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Savings strategy', 'Financial planning']\n",
      "DEBUG: Low-level  keywords: ['Emergency fund', 'Retirement savings', 'Interest rates', 'Budgeting']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query edges: Personal finance, Savings strategy, Financial planning, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.69it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 13 to 13 (max tokens:4000)\n",
      "INFO: Global query uses 13 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 1742\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 18.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 12 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:hybrid type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:hybrid type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode global: **Savings Rate Recommendation**\n",
      "\n",
      "According to the provided Knowledge Base, the recommended savings rate is 20% of the monthly salary. This is mentioned in the \"Financial Planning Overview\" source, which suggests setting aside 20% of the monthly salary for savings.\n",
      "\n",
      "**Contextual Information**\n",
      "\n",
      "It's essential to note that this savings rate is part of a comprehensive financial planning approach that involves budgeting, tracking expenses, creating an emergency fund, and balancing necessary expenses with leisure activities. Additionally, the Knowledge Base emphasizes the importance of reviewing and adjusting the budget quarterly to ensure alignment with long-term financial goals.\n",
      "\n",
      "If you have any further questions or would like more information on financial planning or budgeting, feel free to ask!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Savings strategy', 'Financial planning']\n",
      "DEBUG: Low-level  keywords: ['Emergency fund', 'Retirement savings', 'Interest rates', 'Budgeting']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query nodes: Emergency fund, Retirement savings, Interest rates, Budgeting, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.43it/s]\n",
      "INFO: Query edges: Personal finance, Savings strategy, Financial planning, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.49it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 13 to 13 (max tokens:4000)\n",
      "INFO: Global query uses 13 entites, 15 relations, 1 chunks\n",
      "DEBUG: Truncate relations from 15 to 15 (max tokens:4000)\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 9 to 9 (max tokens:4000)\n",
      "INFO: Local query uses 9 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 2656\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 27.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 14 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:naive type:query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode hybrid: **Savings Rate Recommendation**\n",
      "\n",
      "According to the provided Financial Planning Overview, it is recommended to set aside **20% of monthly salary for savings**. This is an essential step in managing personal finances effectively and achieving long-term financial goals.\n",
      "\n",
      "It's worth noting that this savings rate is not only for emergency funds but also for other long-term investments and goals. By allocating a significant portion of your income towards savings, you can ensure a stable financial future and achieve your objectives.\n",
      "\n",
      "Remember, this is a general guideline, and the ideal savings rate may vary depending on individual circumstances. It's essential to review and adjust your budget regularly to ensure you're on track to meet your financial goals.\n",
      "\n",
      "Query: How should I track my expenses?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.27it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: [naive_query]Prompt Tokens: 332\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 15 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:local type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:local type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode naive: **Tracking Expenses for Effective Financial Planning**\n",
      "\n",
      "Tracking daily expenses is a crucial step in managing your personal finances effectively. As mentioned in the financial planning overview, understanding your spending patterns is essential to create a realistic budget. \n",
      "\n",
      "**Digital Tools for Expense Tracking**\n",
      "\n",
      "To make expense tracking easier, consider using digital tools. These tools can help you monitor your daily expenses, categorize them, and provide insights into your spending habits. With digital tools, you can easily track your expenses on-the-go, making it a convenient and efficient way to stay on top of your finances.\n",
      "\n",
      "**Why Track Expenses?**\n",
      "\n",
      "By tracking your expenses, you'll be able to identify areas where you can cut back and allocate your money more wisely. This will help you create a balanced budget that covers both necessary expenses and leisure activities. Additionally, tracking expenses will enable you to set aside 20% of your monthly salary for savings and create an emergency fund that covers 6 months of expenses.\n",
      "\n",
      "Remember to review and adjust your budget quarterly to ensure you're on track with your financial goals. By following these steps, you'll be well on your way to achieving effective financial planning and securing a stable financial future.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Expense management', 'Budgeting']\n",
      "DEBUG: Low-level  keywords: ['Transaction tracking', 'Budgeting apps', 'Expense categorization', 'Financial planning']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query nodes: Transaction tracking, Budgeting apps, Expense categorization, Financial planning, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.61it/s]\n",
      "DEBUG: Truncate relations from 15 to 15 (max tokens:4000)\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 12 to 12 (max tokens:4000)\n",
      "INFO: Local query uses 12 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 1408\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 17.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 17 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:global type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:global type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode local: **Tracking Expenses: A Crucial Step in Budgeting**\n",
      "\n",
      "Tracking expenses is an essential part of budgeting, as it helps you understand your spending patterns and identify areas where you can cut back. According to the Financial Planning Overview, tracking daily expenses is a crucial step in managing personal finances effectively.\n",
      "\n",
      "**Digital Tools for Expense Tracking**\n",
      "\n",
      "One effective way to track your expenses is by using digital tools. These tools can help you monitor your spending, categorize your expenses, and provide insights into your financial habits. As mentioned in the relationships between Digital Tools and Financial Planning Overview, digital tools are used in financial planning to track expenses, manage budgets, and plan finances.\n",
      "\n",
      "**Categorizing Expenses**\n",
      "\n",
      "When tracking your expenses, it's essential to categorize them into necessary expenses, such as rent, utilities, and food, and leisure activities, such as hobbies or travel. This will help you balance your necessary expenses with your leisure activities, as mentioned in the relationship between Budgeting and Leisure Activities.\n",
      "\n",
      "**Quarterly Review**\n",
      "\n",
      "Remember to review and adjust your budget quarterly, as mentioned in the relationship between Financial Planning Overview and Quarterly Review. This will help you stay on track with your financial goals and make adjustments as needed.\n",
      "\n",
      "By following these steps, you'll be able to effectively track your expenses and take control of your finances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Expense management', 'Budgeting']\n",
      "DEBUG: Low-level  keywords: ['Transaction tracking', 'Budgeting apps', 'Expense categorization', 'Financial planning']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query edges: Personal finance, Expense management, Budgeting, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.48it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 13 to 13 (max tokens:4000)\n",
      "INFO: Global query uses 13 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 1742\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 18.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 19 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:hybrid type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:hybrid type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode global: **Tracking Expenses: A Crucial Step in Budgeting**\n",
      "\n",
      "Tracking expenses is an essential part of budgeting, as it helps you understand your spending patterns and identify areas where you can cut back. According to the Financial Planning Overview, tracking daily expenses is a crucial step in managing personal finances effectively.\n",
      "\n",
      "**Digital Tools for Expense Tracking**\n",
      "\n",
      "You can use digital tools to track your expenses. These tools can help you manage your budget, track your spending, and plan your finances. Digital tools are an essential part of financial planning, and they can make it easier for you to stay on top of your expenses.\n",
      "\n",
      "**Budgeting and Expense Allocation**\n",
      "\n",
      "Budgeting involves allocating your resources towards necessary expenses, such as rent, utilities, and food. It's essential to balance these necessary expenses with leisure activities, such as hobbies or travel. By tracking your expenses, you can ensure that you're allocating your resources effectively and making the most of your money.\n",
      "\n",
      "**Quarterly Review: Assessing Your Financial Situation**\n",
      "\n",
      "Regularly reviewing your financial situation, budget, and goals is crucial. A quarterly review can help you assess your spending patterns, identify areas for improvement, and make adjustments to your budget as needed.\n",
      "\n",
      "Remember, tracking your expenses is a crucial step in budgeting and financial planning. By using digital tools and regularly reviewing your financial situation, you can take control of your finances and achieve your long-term financial goals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Personal finance', 'Expense management', 'Budgeting']\n",
      "DEBUG: Low-level  keywords: ['Transaction tracking', 'Budgeting apps', 'Expense categorization', 'Financial planning']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query nodes: Transaction tracking, Budgeting apps, Expense categorization, Financial planning, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.67it/s]\n",
      "INFO: Query edges: Personal finance, Expense management, Budgeting, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 13 to 13 (max tokens:4000)\n",
      "INFO: Global query uses 13 entites, 15 relations, 1 chunks\n",
      "DEBUG: Truncate relations from 15 to 15 (max tokens:4000)\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 12 to 12 (max tokens:4000)\n",
      "INFO: Local query uses 12 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 2756\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 29.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 21 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:naive type:query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode hybrid: **Tracking Expenses: A Crucial Step in Budgeting**\n",
      "\n",
      "Tracking expenses is an essential aspect of budgeting, as it helps you understand your spending patterns and identify areas where you can cut back. According to the Financial Planning Overview, tracking daily expenses is crucial for managing personal finances effectively.\n",
      "\n",
      "**Methods for Tracking Expenses**\n",
      "\n",
      "There are several ways to track your expenses, including:\n",
      "\n",
      "* **Manual Tracking**: Write down every single transaction in a notebook or journal. This method can be time-consuming, but it helps you stay aware of your spending habits.\n",
      "* **Digital Tools**: Utilize digital tools such as budgeting apps, spreadsheets, or online software to track your expenses. These tools can categorize your expenses, provide insights, and offer alerts and reminders.\n",
      "* **Receipts and Invoices**: Keep receipts and invoices for all your transactions, and file them accordingly. This method is useful for tracking specific expenses, such as business expenses or medical bills.\n",
      "\n",
      "**Tips for Effective Expense Tracking**\n",
      "\n",
      "* **Be Consistent**: Track your expenses regularly, ideally on a daily basis.\n",
      "* **Categorize Expenses**: Divide your expenses into categories, such as necessary expenses (rent, utilities), leisure activities (entertainment, hobbies), and savings.\n",
      "* **Set Budget Goals**: Determine your budget goals and allocate your income accordingly.\n",
      "* **Review and Adjust**: Regularly review your expenses and adjust your budget as needed.\n",
      "\n",
      "**Using Digital Tools for Expense Tracking**\n",
      "\n",
      "Digital tools can simplify the expense tracking process and provide valuable insights into your spending habits. Some popular digital tools for expense tracking include budgeting apps, spreadsheets, and online software. These tools can help you:\n",
      "\n",
      "* **Automate Expense Tracking**: Connect your bank accounts and credit cards to automatically track your expenses.\n",
      "* **Categorize Expenses**: Categorize your expenses into different groups, such as necessary expenses, leisure activities, and savings.\n",
      "* **Set Budget Alerts**: Receive alerts and reminders when you go over budget or reach specific spending thresholds.\n",
      "* **Analyze Spending Patterns**: Analyze your spending patterns and identify areas where you can cut back.\n",
      "\n",
      "By tracking your expenses effectively, you can gain control over your finances, make informed budgeting decisions, and achieve your long-term financial goals.\n",
      "\n",
      "Query: What is the emergency fund recommendation?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.24it/s]\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: [naive_query]Prompt Tokens: 332\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n",
      "INFO: Inserting 1 records to llm_response_cache\n",
      "INFO: Process 11020 KV writting 22 records to llm_response_cache\n",
      "DEBUG: Non-embedding cached missed(mode:local type:query)\n",
      "DEBUG: Non-embedding cached missed(mode:local type:keywords)\n",
      "DEBUG: [kg_query]Prompt Tokens: 417\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode naive: **Emergency Fund Recommendation**\n",
      "\n",
      "According to the financial planning overview, it is recommended to create an emergency fund that covers **6 months of expenses**. This fund serves as a safety net to ensure that you are prepared for any unexpected events or financial setbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Inserting 1 records to llm_response_cache\n",
      "DEBUG: High-level keywords: ['Financial planning', 'Emergency preparedness', 'Savings strategy']\n",
      "DEBUG: Low-level  keywords: ['Three-month rule', 'Expenses coverage', 'Liquid assets', 'Cash reserve']\n",
      "INFO: Process 11020 buidling query context...\n",
      "INFO: Query nodes: Three-month rule, Expenses coverage, Liquid assets, Cash reserve, top_k: 60, cosine: 0.2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.74it/s]\n",
      "DEBUG: Truncate relations from 15 to 15 (max tokens:4000)\n",
      "DEBUG: Truncate chunks from 1 to 1 (max tokens:4000)\n",
      "DEBUG: Truncate entities from 11 to 11 (max tokens:4000)\n",
      "INFO: Local query uses 11 entites, 15 relations, 1 chunks\n",
      "DEBUG: [kg_query]Prompt Tokens: 1378\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 17.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.llama_index_impl import llama_index_complete, llama_index_embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.utils import setup_logger, EmbeddingFunc\n",
    "from lightrag import LightRAG, QueryParam\n",
    "\n",
    "# Enable debug logging\n",
    "setup_logger(\"lightrag\", level=\"DEBUG\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Sample content to write to facts.txt\n",
    "SAMPLE_CONTENT = \"\"\"\n",
    "Financial Planning Overview:\n",
    "1. Budgeting is essential for managing personal finances effectively.\n",
    "2. Track daily expenses to understand spending patterns.\n",
    "3. Set aside 20% of monthly salary for savings.\n",
    "4. Create emergency fund covering 6 months of expenses.\n",
    "5. Balance between necessary expenses and leisure activities.\n",
    "6. Review and adjust budget quarterly.\n",
    "7. Use digital tools for expense tracking.\n",
    "8. Consider long-term financial goals in planning.\n",
    "\"\"\"\n",
    "\n",
    "async def initialize_rag():\n",
    "    llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    async def llm_wrapper(query, **kwargs):\n",
    "        try:\n",
    "            return await llama_index_complete(\n",
    "                query,\n",
    "                llm_instance=llm,\n",
    "                **kwargs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=\"R:/lightrag_implemented_chatbot\",\n",
    "        llm_model_func=llm_wrapper,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=384,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    return rag\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        # Write sample content to facts.txt\n",
    "        with open(\"facts.txt\", \"w\") as f:\n",
    "            f.write(SAMPLE_CONTENT)\n",
    "        \n",
    "        rag = await initialize_rag()\n",
    "        \n",
    "        # Load and verify content\n",
    "        with open(\"facts.txt\", \"r\") as f:\n",
    "            content = f.read()\n",
    "            print(f\"Loading content ({len(content)} characters)...\")\n",
    "            rag.insert(content)\n",
    "            print(\"Content inserted successfully\")\n",
    "\n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"What is the recommended savings rate?\",\n",
    "            \"How should I track my expenses?\",\n",
    "            \"What is the emergency fund recommendation?\"\n",
    "        ]\n",
    "\n",
    "        for query in test_queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            for mode in [\"naive\", \"local\", \"global\", \"hybrid\"]:\n",
    "                try:\n",
    "                    result = await rag.aquery(query, param=QueryParam(mode=mode))\n",
    "                    print(f\"Mode {mode}: {result}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {mode} mode: {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
